from pydantic import BaseModel, Field
from typing import Optional, Any, List, Type
from textwrap import dedent
from typing import List
import asyncio
import json
import os


from genai_core.telemetry import instrumented_trace, TraceInstruments
from genai_core.prompts import create_prompt_list
from genai_core.logging import logging
from genai_core.tools import BaseTool
from genai.tools import SonarConfigurations, SonarDomains, ToolSonarAnalysis
from genai.tools import ToolChatLLM, GitFunctionalities

logger = logging.getLogger()

class TaskInput(BaseModel):
    project_name: str = Field(description="Repository Name")
    domains: List[SonarDomains] = Field(description="List Sonar Domains")

class TaskSonarqube(BaseTool):
    name = "Task Sonarqube Metrics"
    description = "useful for when you need to generate sonarqube metrics in specify repositories"
    args_schema: Type[BaseModel] = TaskInput
    url = os.environ['SONAR_HOST']
    token = os.environ['SONAR_TOKEN']
    tool_sonar_analysis = ToolSonarAnalysis()
    
    @instrumented_trace()
    async def _run(self, project_name, domains):
        for domain in domains:
            
            logger.info(f"Selecting domain=`{domain}`")
            analysis = await self.tool_sonar_analysis.run(tool_input={"project_name": project_name,
                                                                 "token": self.token,
                                                                 "url": self.url,
                                                                 "domain": domain})
            logger.debug(analysis)
            json_data = self.get_info_from_json_file(domain.value)
            self.domain_name, self.domain_description = self.get_domain(json_data)
            await asyncio.sleep(0)
            metrics_description = self.extract_keys_and_descriptions(json_data)
            metrics_results = analysis
            
            prompt = self.add_prompts(
                self.domain_name, 
                self.domain_description,
                metrics_description=metrics_description, 
                metrics_results=metrics_results
            )
            await self._create_chat(prompt)

    @instrumented_trace(span_name="Add Prompts Template")
    def add_prompts(self, domain_name, domain_description, metrics_description, metrics_results):
        prompt = create_prompt_list([
            {
                "role": "system", 
                "content": dedent("""
                    You are a senior software engineer.
                    Your skill is to analyze metrics using the Sonarqube tool to provide valuable insights on areas that need improvement in a given application.
                    The metrics available belong to the {domain_name} domain. Please refer to the domain description to understand the context of the metrics: {domain_description}\n.
                """),
                "parameters": {
                    'domain_name': domain_name, 
                    'domain_description': domain_description
                }
            },
            {
                "role": "user", 
                "content": dedent("""
                    The [Description] section contains descriptions of each metric.
                    The [Metrics] section contains the metrics generated by the tool for a particular repository.
                        
                    [Description]
                    {metrics_description}\n\n
                        
                    [Metrics]
                    {metrics_results}\n
                """),
                "parameters": {
                    'metrics_description': metrics_description, 
                    'metrics_results': metrics_results
                }
            }
        ])
        
        return prompt

    @instrumented_trace(span_name="Load Domain Json", type=TraceInstruments.INSTRUMENTS_EVENT)
    def get_info_from_json_file(self, metric_json) -> dict:
        path_file_metric_json = f"{SonarConfigurations.PATH_METRICS}/{metric_json}"
        with open(path_file_metric_json, 'r') as arquivo:
            json_data = json.load(arquivo)
        return json_data

    @instrumented_trace(span_name="Reading METRIC and DESCRIPTION")
    def extract_keys_and_descriptions(self, json_data):
        logger.info("Reading METRIC and DESCRIPTION informations")
        metrics = json_data["Metrics"]
        result = "\n"
        for metric in metrics:
            result += f"- {metric['key']}: {metric['description']}\n"
        return result

    @instrumented_trace(span_name="Reading DOMAIN and CONTEXT", type=TraceInstruments.INSTRUMENTS_EVENT)
    def get_domain(self, json_data):
        logger.info("Reading DOMAIN and CONTEXT informations")
        return json_data['Domain'], json_data['Context']

    @instrumented_trace(span_name="Creating Chat", kind=TraceInstruments.SPAN_KIND_CLIENT, span_parameters=False)
    async def _create_chat(self, prompt):
        logger.debug("Create chat")

        chat = await ToolChatLLM().run(tool_input={"model": os.environ['OPENAI_MODEL_NAME'], 
                                             "api_key": os.environ['OPENAI_API_KEY'],
                                             "api_base": os.environ['OPENAI_BASE_URL'],
                                             "prompt": prompt,
                                             "streaming": False})
        
        logger.debug(chat)
    
