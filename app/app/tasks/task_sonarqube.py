from langchain.prompts import PromptTemplate
from app.agents.create_chat import create_chat
from langchain_core.callbacks import BaseCallbackHandler
from app.configs import logging
import json
import os
# from langfuse.decorators import observe, langfuse_context
# from langfuse.openai import openai
from typing import List

from app.prompts import create_prompt_list
from app.telemetry import instrumented_trace
from textwrap import dedent
from app.configs import logging
from app.utils import parser_config_toml_files, paths

# tools_settings = (
#     app_settings.properties |
#     parser_config_toml_files.run(
#         config_name='tools', 
#         dir_path="/workspaces/gen-ai-tutorials/app/app/tools/configs-toml"
#     ) |
#     parser_config_toml_files.run(
#         config_name='tools-sonar', 
#         dir_path="/workspaces/gen-ai-tutorials/app/app/tools/configs-toml"
#     )
# )

# DIR_PATH_METRICS = "/workspaces/gen-ai-tutorials/app/tools/sonarqube/metrics"
# METRICS = tools_settings['tools']['sonar']['analysis']['METRICS']

DIR_PATH_METRICS = ""
METRICS = ""

logger = logging.getLogger()

class TaskSonarqube:
    
    def __init__(self, tools_repos, tools_analysis, tools_scanners, 
                 project_name: str, url_repo: str, sonar_token: str, sonar_url: str,
                 metric_name: str = "",
                 metric_list: List[str] = [],
                 callbacks: BaseCallbackHandler = None):
        self.tools_repos = tools_repos
        self.tools_analysis = tools_analysis
        self.tools_scanners = tools_scanners
        self.project_name = project_name
        self.url_repo = url_repo
        self.sonar_token = sonar_token
        self.sonar_url = sonar_url
        self.metrics_name = metric_name
        self.callbacks = callbacks
        self.add_metrics(metric_json_items=metric_list)

    def _run(self):
        # self.tools_repos.run(tool_input={"project_name": self.project_name, "url": self.url_repo,
        #                                  "function": 'git_clone'},
        #                      callbacks=self.callbacks)

        # self.tools_scanners.run(tool_input={"project_name": self.project_name, "token": self.sonar_token, 
        #                                   "url": self.sonar_url},
        #                         callbacks=self.callbacks)
        try:
            metric_name = self.get_metric_json()
            self.analysis = self.tools_analysis.run(tool_input={"project_name": self.project_name, "token": self.sonar_token,
                                                "url": self.sonar_url, "metric_name": metric_name},
                                                    callbacks=self.callbacks)
            
            json_data = self.get_info_from_json_file(metric_name)
            self.domain_name, self.domain_description = self.get_domain(json_data)
            metrics_description = self.extract_keys_and_descriptions(json_data)
            metrics_results = self.analysis
            
            logger.info("Iniciando chat")
            logger.debug(dedent(f"""
                Domain: {self.domain_name}
                Description: {self.domain_description}
                Metrics Description: 
                {metrics_description}
                Metrics: 
                {metrics_results}
            """))
            self.add_prompts(
                self.domain_name, 
                self.domain_description,
                metrics_description=metrics_description, 
                metrics_results=metrics_results
            )
            self._create_chat()
            
            self._run()
        except StopIteration:
            logger.debug("No metrics")

    def add_prompts(self, domain_name, domain_description, metrics_description, metrics_results):
        self.prompts = create_prompt_list([
            {
                "role": "system", 
                "content": dedent("""
                    You are a senior software engineer.
                    Your skill is to analyze metrics using the Sonarqube tool to provide valuable insights on areas that need improvement in a given application.
                    The metrics available belong to the {domain_name} domain. Please refer to the domain description to understand the context of the metrics: {domain_description}\n.
                """),
                "parameters": {
                    'domain_name': domain_name, 
                    'domain_description': domain_description
                }
            },
            {
                "role": "user", 
                "content": dedent("""
                    The [Description] section contains descriptions of each metric.
                    The [Metrics] section contains the metrics generated by the tool for a particular repository.
                        
                    [Description]
                    {metrics_description}\n\n
                        
                    [Metrics]
                    {metrics_results}\n
                """),
                "parameters": {
                    'metrics_description': metrics_description, 
                    'metrics_results': metrics_results
                }
            }
        ])

    def get_info_from_json_file(self, metric_json) -> dict:
        path_file_metric_json = f"{DIR_PATH_METRICS}/{metric_json}.json"
        with open(path_file_metric_json, 'r') as arquivo:
            json_data = json.load(arquivo)
        return json_data

    def extract_keys_and_descriptions(self, json_data):
        logger.info("Reading METRIC and DESCRIPTION informations")
        metrics = json_data["Metrics"]
        result = "\n"
        for metric in metrics:
            result += f"- {metric['key']}: {metric['description']}\n"
        return result

    def add_metrics(self, metric_json_items: List[str]):
        logger.info("Adding metrics to evaluate list")
        self.metrics_json_list = metric_json_items
        if not self.metrics_json_list:
            self.metrics_json_list = [f"{item}" for item in METRICS]
        else:
            self.metrics_json_list = [f"{item}" for item in self.metrics_json_list]
        self.metrics_json_list = iter(self.metrics_json_list)

    def get_metric_json(self):
        metric_json_item = next(self.metrics_json_list)
        logger.info(f"Selecting metric=`{metric_json_item}`")
        return metric_json_item

    def get_domain(self, json_data):
        logger.info("Reading DOMAIN and CONTEXT informations")
        return json_data['Domain'], json_data['Context']

    # @observe()
    def _create_chat(self):
        logger.debug("Create chat")
        # chat = create_chat(openai_api_key=os.environ['OPENAI_API_KEY'],
        #                    openai_api_base=os.environ['OPENAI_BASE_URL'],
        #                    model=os.environ['OPENAI_MODEL_NAME'],
        #                    temperature=0.3)
        
        # chat.invoke(self.prompts)
        
        #--------------------------------#
        # client = OpenAI(
        #     base_url=os.environ['OPENAI_BASE_URL'], 
        #     api_key=os.environ['OPENAI_API_KEY']
        # )

        # langfuse_context.update_current_trace(
        #     tags=["task_sonarqube", f"domain: {self.domain_name}", f"repo: {self.project_name}"]
        # )
        
        # completion = openai.chat.completions.create(
        #     model=os.environ['OPENAI_MODEL_NAME'],
        #     messages=self.prompts,
        #     temperature=0.3,
        #     user_id=os.environ['LANGFUSE_USER_ID'],
        #     tags=["task-sonarqube"]
        # )
        # logger.info(completion.choices[0].message.content)
    
