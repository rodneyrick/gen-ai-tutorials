from app import configs
import os
from langchain.prompts import PromptTemplate
from app.tasks.sonar_evaluations import SonarEvaluations
from openai import OpenAI
import json
from typing import List
from textwrap import dedent

configs.load_dotenv()
logger = configs.logging.getLogger()

class Task_Sonarqube:
    
    def __init__(self, 
                 org_or_user: str, 
                 repository_name: str,
                 metric_json_items: List[str] = [],
                 **kwargs,
        ):
        self.org_or_user = org_or_user
        self.repository_name = repository_name
        self.sonar_token = os.environ['SONAR_TOKEN']
        self.dir_path_metrics: str = "/workspaces/gen-ai-tutorials/app/tools/sonarqube/metrics"
        self.sonar: SonarEvaluations = None
        self.add_metrics(metric_json_items=metric_json_items)
        # print(os.environ['OPENAI_BASE_URL'])
        # print(os.environ['OPENAI_API_KEY'])

    def add_metrics(self, metric_json_items: List[str]):
        logger.info("Adding metrics to evaluate list")
        self.metrics_json_list = metric_json_items
        if not self.metrics_json_list:
            self.metrics_json_list = os.listdir(self.dir_path_metrics)
        else:
            self.metrics_json_list = [f"{item}.json" for item in self.metrics_json_list]
        self.metrics_json_list = iter(self.metrics_json_list)
        
    def get_metric_json(self):
        metric_json_item = next(self.metrics_json_list)
        logger.info(f"Selecting metric=`{metric_json_item}`")
        return metric_json_item
    
    def prompt_system(self):
        
        template = """
        You are a senior software engineer.
        Your skill is to analyze metrics using the Sonarqube tool to provide valuable insights on areas that need improvement in a given application.
        The metrics available belong to the {domain_name} domain. Please refer to the domain description to understand the context of the metrics: {domain_description}\n.
        """
        
        prompt_template = PromptTemplate.from_template(template)
        return prompt_template
    
    def prompt_user(self):
        template = """
        The [Description] section contains descriptions of each metric.
        The [Metrics] section contains the metrics generated by the tool for a particular repository.
            
        [Description]
        {description}\n\n
            
        [Metrics]
        {metrics}\n
        """
        
        prompt_template = PromptTemplate.from_template(template)
        return prompt_template
    
    def get_info_from_json_file(self, metric_json) -> dict:
        path_file_metric_json = f"{self.dir_path_metrics}/{metric_json}"
        with open(path_file_metric_json, 'r') as arquivo:
            json_data = json.load(arquivo)
        return json_data

    def get_domain(self, json_data):
        logger.info("Reading DOMAIN and CONTEXT informations")
        return json_data['Domain'], json_data['Context']
    
    def extract_keys_and_descriptions(self, json_data):
        logger.info("Reading METRIC and DESCRIPTION informations")
        metrics = json_data["Metrics"]
        result = "\n"
        for metric in metrics:
            result += f"- {metric['key']}: {metric['description']}\n"
        return result
    
    def extract_metrics(self, sonar, metric_json):
        # sonar = SonarEvaluations(
        #     sonar_token=self.sonar_token,
        #     project_name=self.repository_name,
        #     github_url=f"https://github.com/{self.org_or_user}/{self.repository_name}"
        # )
        path_metric_json = f"{self.dir_path_metrics}/{metric_json}"
        evaluation_sonar = sonar.make_evaluation(path_metric_json)

        result = ""
        for evaluation in evaluation_sonar:
            result += f"- {evaluation}\n"
        
        return result
    
    def create_chat(self):

        if self.sonar is None:
            self.sonar = SonarEvaluations(
                sonar_token=self.sonar_token,
                project_name=self.repository_name,
                github_url=f"https://github.com/{self.org_or_user}/{self.repository_name}"
            )

        metric_json = self.get_metric_json()

        # client = OpenAI()
        print(os.environ['OPENAI_BASE_URL'])
        print(os.environ['OPENAI_API_KEY'])
        client = OpenAI(
            base_url=os.environ['OPENAI_BASE_URL'], 
            api_key=os.environ['OPENAI_API_KEY']
        )
        json_data = self.get_info_from_json_file(metric_json)
        domain_name, domain_description = self.get_domain(json_data)
        metrics_description = self.extract_keys_and_descriptions(json_data)
        metrics_results = self.extract_metrics(self.sonar, metric_json)
        
        logger.info("Iniciando chat")
        logger.info(dedent(f"""
            Domain: {domain_name}
            Description: {domain_description}
            Metrics Description: 
            {metrics_description}
            Metrics: 
            {metrics_results}
        """))

        completion = client.chat.completions.create(
            model=os.environ['OPENAI_MODEL_NAME'],
            messages=[
                {"role": "system", 
                 "content": self.prompt_system().format(domain_name=domain_name, 
                                                        domain_description=domain_description)},
                {"role": "user", 
                 "content": self.prompt_user().format(description=metrics_description, 
                                                      metrics=metrics_results)}
            ],
            temperature=0.3
        )
        logger.info(completion.choices[0].message.content)
        return completion.choices[0].message.content
        
    
