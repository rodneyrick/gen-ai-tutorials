from langchain.prompts import PromptTemplate
from app.configs import logging
from openai import OpenAI
import json
import os
from langfuse.decorators import observe, langfuse_context
from langfuse.openai import openai
from textwrap import dedent
from typing import List

DIR_PATH_METRICS = "/workspaces/gen-ai-tutorials/app/tools/sonarqube/metrics"

logger = logging.getLogger()

class TaskSonarqube:
    
    def __init__(self, tools_repos, tools_analysis, tools_scanners, 
                 project_name: str, url_repo: str, sonar_token: str, sonar_url: str,
                 metric_name: str):
        self.tools_repos = tools_repos
        self.tools_analysis = tools_analysis
        self.tools_scanners = tools_scanners
        self.project_name = project_name
        self.url_repo = url_repo
        self.sonar_token = sonar_token
        self.sonar_url = sonar_url
        self.metrics_name = metric_name
        # self.add_metrics(metric_json_items=metric_json_items)
        
    def _run(self):
        self.tools_repos.run(tool_input={"project_name": self.project_name, "url": self.url_repo,
                                         "function": 'git_clone'})

        self.tools_scanners.run(tool_input={"project_name": self.project_name, "token": self.sonar_token, 
                                          "url": self.sonar_url})
        
        self.analysis = self.tools_analysis.run(tool_input={"project_name": self.project_name, "token": self.sonar_token,
                                             "url": self.sonar_url, "metric_name": self.metrics_name})
            
        self.create_chat()
                  
    def prompt_system(self):
        
        template = """
        You are a senior software engineer.
        Your skill is to analyze metrics using the Sonarqube tool to provide valuable insights on areas that need improvement in a given application.
        The metrics available belong to the {domain_name} domain. Please refer to the domain description to understand the context of the metrics: {domain_description}\n.
        """
        
        prompt_template = PromptTemplate.from_template(template)
        return prompt_template
    
    def prompt_user(self):
        template = """
        The [Description] section contains descriptions of each metric.
        The [Metrics] section contains the metrics generated by the tool for a particular repository.
            
        [Description]
        {description}\n\n
            
        [Metrics]
        {metrics}\n
        """
        
        prompt_template = PromptTemplate.from_template(template)
        return prompt_template

    def get_info_from_json_file(self, metric_json) -> dict:
        path_file_metric_json = f"{DIR_PATH_METRICS}/{metric_json}.json"
        with open(path_file_metric_json, 'r') as arquivo:
            json_data = json.load(arquivo)
        return json_data

    def extract_keys_and_descriptions(self, json_data):
        logger.info("Reading METRIC and DESCRIPTION informations")
        metrics = json_data["Metrics"]
        result = "\n"
        for metric in metrics:
            result += f"- {metric['key']}: {metric['description']}\n"
        return result

    def add_metrics(self, metric_json_items: List[str]):
        logger.info("Adding metrics to evaluate list")
        self.metrics_json_list = metric_json_items
        if not self.metrics_json_list:
            self.metrics_json_list = os.listdir(DIR_PATH_METRICS)
        else:
            self.metrics_json_list = [f"{item}.json" for item in self.metrics_json_list]
        self.metrics_json_list = iter(self.metrics_json_list)

    def get_metric_json(self):
        metric_json_item = next(self.metrics_json_list)
        logger.info(f"Selecting metric=`{metric_json_item}`")
        return metric_json_item

    def get_domain(self, json_data):
        logger.info("Reading DOMAIN and CONTEXT informations")
        return json_data['Domain'], json_data['Context']

    @observe()
    def create_chat(self):

        # client = OpenAI(
        #     base_url=os.environ['OPENAI_BASE_URL'], 
        #     api_key=os.environ['OPENAI_API_KEY']
        # )
        
        # metric_json = self.get_metric_json()
        json_data = self.get_info_from_json_file(self.metrics_name)
        domain_name, domain_description = self.get_domain(json_data)
        metrics_description = self.extract_keys_and_descriptions(json_data)
        metrics_results = self.analysis
        
        logger.info("Iniciando chat")
        logger.info(dedent(f"""
            Domain: {domain_name}
            Description: {domain_description}
            Metrics Description: 
            {metrics_description}
            Metrics: 
            {metrics_results}
        """))

        langfuse_context.update_current_trace(
            tags=["task_sonarqube", f"domain: {domain_name}", f"repo: {self.project_name}"]
        )
        
        completion = openai.chat.completions.create(
            model=os.environ['OPENAI_MODEL_NAME'],
            messages=[
                {"role": "system", 
                 "content": self.prompt_system().format(domain_name=domain_name, 
                                                        domain_description=domain_description)},
                {"role": "user", 
                 "content": self.prompt_user().format(description=metrics_description, 
                                                      metrics=metrics_results)}
            ],
            temperature=0.3
        )
        logger.info(completion.choices[0].message.content)
        # return completion.choices[0].message.content
    
